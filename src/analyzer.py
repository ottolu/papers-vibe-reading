"""Gemini 3.0 Pro Vibe Reading â€” analyse papers via the Google GenAI SDK."""

from __future__ import annotations

import asyncio
import json
import logging
from datetime import date
from pathlib import Path
from typing import TYPE_CHECKING

from google import genai
from google.genai import types
import httpx

from . import config

if TYPE_CHECKING:
    from .fetcher import Paper

logger = logging.getLogger(__name__)

VIBE_READING_PROMPT = """\
ä½ æ˜¯ä¸€ä½é¡¶çº§çš„ AI researcherï¼Œç²¾é€šå­¦æœ¯å†…å®¹è§£è¯»ä¸æ•°æ®å¯è§†åŒ–ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ä¸€ç¯‡å¤æ‚çš„å­¦æœ¯è®ºæ–‡ï¼Œè½¬åŒ–ä¸ºä¸€ä»½ç¬¦åˆèƒ½è®©è¯»è€…é«˜æ•ˆã€å¿«é€ŸæŒæ¡æ–‡ç« æ ¸å¿ƒå†…å®¹ã€åŸç†å’Œåˆ›æ–°ç‚¹çš„é˜…è¯»ææ–™ã€‚
è¯·å°†ä¸Šä¼ çš„æŒ‡å®šå­¦æœ¯è®ºæ–‡ï¼ŒæŒ‰ç…§è¦æ±‚ç”Ÿæˆä¸€ä»½èƒ½è®©è¯»è€…é«˜æ•ˆã€å¿«é€ŸæŒæ¡æ–‡ç« æ ¸å¿ƒå†…å®¹ã€åŸç†å’Œåˆ›æ–°ç‚¹çš„é˜…è¯»ææ–™ï¼Œå…¶ä¸­éœ€æ·±åº¦è§£æå¹¶é‡ç‚¹å±•ç¤ºè®ºæ–‡çš„
- **ç ”ç©¶åŠ¨æœº**ï¼šå‘ç°äº†ä»€ä¹ˆé—®é¢˜ï¼Œä¸ºä»€ä¹ˆéœ€è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡ç ”ç©¶çš„ significance æ˜¯ä»€ä¹ˆ
- **æ•°å­¦è¡¨ç¤ºåŠå»ºæ¨¡**ï¼šä»ç¬¦å·/è¡¨ç¤ºåˆ°å…¬å¼ï¼Œä»¥åŠå…¬å¼æ¨å¯¼å’Œç®—æ³•æµç¨‹ï¼Œæ³¨æ„æ”¯æŒ latex çš„æ¸²æŸ“
- **å®éªŒæ–¹æ³•ä¸å®éªŒè®¾è®¡**ï¼šç³»ç»Ÿæ€§æ•´ç†å®éªŒç»†èŠ‚ï¼ˆæ¯”å¦‚æ¨¡å‹ã€æ•°æ®ã€è¶…å‚æ•°ã€promptç­‰ï¼‰ï¼Œå°½å¯èƒ½å‚è€ƒ appendixï¼Œè¾¾åˆ°å¯å¤ç°çš„ç¨‹åº¦ï¼›
- **å®éªŒç»“æœåŠæ ¸å¿ƒç»“è®º**ï¼šå¯¹æ¯”äº†é‚£äº›baselineï¼Œè¾¾åˆ°äº†ä»€ä¹ˆæ•ˆæœï¼Œæ­ç¤ºäº†ä»€ä¹ˆç»“è®ºå’Œinsights
- **ä½ çš„è¯„è®º**ï¼šä½œä¸ºä¸€ä¸ªçŠ€åˆ©çš„reviewerï¼Œæ•´ä½“é”è¯„ä¸‹è¿™ç¯‡å·¥ä½œï¼Œä¼˜åŠ¿ä¸ä¸è¶³ï¼Œä»¥åŠå¯èƒ½çš„æ”¹è¿›æ–¹å‘
- **æ€è€ƒé¢˜**: æå‡ºä¸‰ä¸ªåŸºäºè¿™ç¯‡æ–‡ç« çš„æ€è€ƒé—®é¢˜ï¼Œéš¾åº¦å±‚å±‚é€’è¿›ï¼Œè€ƒå¯Ÿè¯»è€…å¯¹è¿™ç¯‡æ–‡ç« çš„ç†è§£ã€‚
- **One More Thing**: ä½ ä¹Ÿå¯ä»¥è‡ªç”±å‘æŒ¥æœ¬æ–‡ä¸­å…¶ä»–ä½ è®¤ä¸ºé‡è¦ã€å¸Œæœ›åˆ†äº«ç»™æˆ‘çš„å†…å®¹
æ³¨æ„ï¼š
1. æ‰€æœ‰çš„ç¬¦å·åŠå…¬å¼ï¼Œéƒ½è¦èƒ½æ”¯æŒæ­£ç¡®è¿›è¡Œ latex æ¸²æŸ“ï¼ˆä¸åªæ˜¯å…¬å¼å—ï¼Œè¿˜åŒ…æ‹¬inlineçš„å…¬å¼ï¼Œæ³¨æ„**è¡Œå†…å…¬å¼ä¸è¦æ¢è¡Œ**ï¼‰ï¼›
2. é™¤å…¬å¼ä»¥åŠä¸€äº›æ ¸å¿ƒæœ¯è¯­å’ŒæŠ€æœ¯åè¯å¤–ï¼Œå°½å¯èƒ½ç”¨ä¸­æ–‡ã€‚
3. figure/table æ’å…¥æ—¶ï¼Œç”¨è®ºæ–‡ä¸­å…·ä½“çš„ figure/table æ¥è¡¨ç¤ºã€‚ç‰¹åˆ«çš„ï¼Œå¯¹äºå›¾ç‰‡ï¼Œå¦‚æœæ— æ³•ç›´æ¥æ”¾åˆ°ç½‘é¡µä¸­ï¼Œå°±ä½¿ç”¨å ä½ç¬¦è¡¨ç¤ºï¼Œæ–¹ä¾¿æ£€ç´¢ï¼›å¯¹äºè¡¨æ ¼ï¼Œå¦‚æœæ˜¯å…³é”®å®éªŒç›¸å…³è¡¨æ ¼ åˆ™æŒ‰ç…§latexæ ¼å¼è¿›è¡Œæ¸²æŸ“ï¼Œå°†è¡¨æ ¼å†…å…·ä½“å†…å®¹æ”¾åˆ°ç½‘é¡µä¸­ã€‚
4. è¦å°½å¯èƒ½åœ°äº‹æ— å·¨ç»†ï¼Œç›®æ ‡æ˜¯è¯»å®Œè¿™ä¸ªææ–™ï¼ŒåŸºæœ¬æŠŠæ¡äº†è®ºæ–‡90%çš„å†…å®¹äº†ï¼Œå¯ä»¥è¾¾åˆ°å¤ç°è®ºæ–‡çš„ç¨‹åº¦ã€‚
"""

GEMINI_LOG_DIR = Path(config.OUTPUT_DIR) / "gemini_logs"


def _build_client() -> genai.Client:
    """Create a Google GenAI client, routing through the configured proxy.

    Forces HTTP/1.1 when a proxy is configured â€” Clash proxies often fail
    the TLS/ALPN negotiation for HTTP/2, causing ``SSL: UNEXPECTED_EOF``.
    """
    proxy_url = config.get_proxy_url()
    if proxy_url:
        transport = httpx.HTTPTransport(
            proxy=proxy_url,
            http1=True,
            http2=False,
        )
        http_client = httpx.Client(transport=transport, timeout=300)
        return genai.Client(
            api_key=config.GEMINI_API_KEY,
            http_options=types.HttpOptions(httpxClient=http_client),
        )
    return genai.Client(api_key=config.GEMINI_API_KEY)


# ---------------------------------------------------------------------------
# Gemini call logging
# ---------------------------------------------------------------------------

def _log_dir(target_date: date) -> Path:
    """Return ``output/gemini_logs/YYYY-MM-DD/``."""
    d = GEMINI_LOG_DIR / target_date.isoformat()
    d.mkdir(parents=True, exist_ok=True)
    return d


def _save_request_log(
    arxiv_id: str,
    target_date: date,
    user_prompt: str,
    has_pdf: bool,
) -> None:
    """Persist the request we are about to send (everything except the binary PDF)."""
    safe = arxiv_id.replace("/", "_")
    path = _log_dir(target_date) / f"{safe}_request.json"
    payload = {
        "arxiv_id": arxiv_id,
        "model": config.GEMINI_MODEL,
        "contents_structure": {
            "role": "user",
            "parts": (
                ["Part(inline_data=PDF)", "Part(text=user_prompt)"]
                if has_pdf
                else ["Part(text=user_prompt)"]
            ),
        },
        "has_pdf_attachment": has_pdf,
        "user_prompt": user_prompt,
    }
    path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
    logger.info("[%s] Request log â†’ %s", arxiv_id, path)


def _save_response_log(
    arxiv_id: str,
    target_date: date,
    response: types.GenerateContentResponse,
    analysis: str,
) -> None:
    """Persist the API response metadata + full analysis text."""
    safe = arxiv_id.replace("/", "_")
    path = _log_dir(target_date) / f"{safe}_response.json"

    # Extract usage metadata safely
    usage: dict = {}
    if response.usage_metadata:
        um = response.usage_metadata
        usage = {
            "prompt_tokens": getattr(um, "prompt_token_count", None),
            "candidates_tokens": getattr(um, "candidates_token_count", None),
            "total_tokens": getattr(um, "total_token_count", None),
        }

    # Extract finish reason
    finish_reason = None
    if response.candidates:
        fr = getattr(response.candidates[0], "finish_reason", None)
        finish_reason = str(fr) if fr else None

    payload = {
        "arxiv_id": arxiv_id,
        "model": config.GEMINI_MODEL,
        "finish_reason": finish_reason,
        "usage": usage,
        "analysis_length": len(analysis),
        "analysis": analysis,
    }
    path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
    logger.info("[%s] Response log â†’ %s", arxiv_id, path)


def _save_error_log(
    arxiv_id: str,
    target_date: date,
    error: Exception,
) -> None:
    """Persist error information when the API call fails."""
    safe = arxiv_id.replace("/", "_")
    path = _log_dir(target_date) / f"{safe}_error.json"
    payload = {
        "arxiv_id": arxiv_id,
        "error_type": type(error).__name__,
        "error_message": str(error),
    }
    path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
    logger.info("[%s] Error log â†’ %s", arxiv_id, path)


# ---------------------------------------------------------------------------
# Core analysis
# ---------------------------------------------------------------------------

async def analyze_paper(
    paper: "Paper",
    client: genai.Client,
    target_date: date,
) -> str:
    """Analyse a single paper with Gemini.

    The PDF is uploaded as an inline_data Part, followed by the vibe-reading
    instructions as a user text Part.  No system_instruction is used â€” the
    full prompt lives in the user turn so the model treats the PDF + prompt
    as a single coherent request.

    Returns
    -------
    str
        The model's Markdown-formatted analysis.
    """
    parts: list[types.Part] = []

    has_pdf = bool(paper.pdf_bytes)

    if has_pdf:
        # 1) PDF as the first part â€” Gemini sees the full document natively
        parts.append(
            types.Part.from_bytes(data=paper.pdf_bytes, mime_type="application/pdf")
        )
        logger.info(
            "[%s] Attaching PDF (%d bytes, %.1f KB) as inline_data",
            paper.arxiv_id,
            len(paper.pdf_bytes),
            len(paper.pdf_bytes) / 1024,
        )
    else:
        logger.warning(
            "[%s] No PDF available â€” falling back to abstract", paper.arxiv_id
        )

    # 2) Build the user prompt â€” vibe reading instructions + paper title
    #    When no PDF is available, append the abstract text as context.
    user_text = f"è®ºæ–‡æ ‡é¢˜ï¼š{paper.title}\n\n"
    if not has_pdf:
        user_text += f"è®ºæ–‡æ‘˜è¦ï¼š\n{paper.summary}\n\n"
    user_text += VIBE_READING_PROMPT

    user_text = VIBE_READING_PROMPT

    parts.append(types.Part.from_text(text=user_text))

    # Log request
    _save_request_log(paper.arxiv_id, target_date, user_text, has_pdf)

    try:
        response = await asyncio.to_thread(
            client.models.generate_content,
            model=config.GEMINI_MODEL,
            contents=[
                types.Content(role="user", parts=parts),
            ],
            config=types.GenerateContentConfig(
                max_output_tokens=16384,
            ),
        )
        analysis = response.text or ""
        logger.info("[%s] Analysis completed (%d chars)", paper.arxiv_id, len(analysis))

        # Log response
        _save_response_log(paper.arxiv_id, target_date, response, analysis)

        return analysis

    except Exception as exc:
        logger.error("[%s] Gemini API error: %s", paper.arxiv_id, exc)
        # Log error
        _save_error_log(paper.arxiv_id, target_date, exc)
        # Fallback: return a simple summary based on the abstract
        return _fallback_summary(paper)


def _fallback_summary(paper: "Paper") -> str:
    """Generate a minimal summary when the AI call fails."""
    return (
        f"## ğŸ“Œ ä¸€å¥è¯æ€»ç»“\n{paper.title}\n\n"
        f"## ğŸ”‘ æ ¸å¿ƒè´¡çŒ®\nï¼ˆAI åˆ†ææš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·å‚è€ƒåŸæ–‡æ‘˜è¦ï¼‰\n\n"
        f"## ğŸ› ï¸ æ–¹æ³•æ¦‚è¿°\n{paper.summary[:500]}\n\n"
        f"## ğŸ“Š å…³é”®ç»“æœ\nï¼ˆè¯·å‚è€ƒåŸæ–‡ï¼‰\n\n"
        f"## ğŸ’¡ ä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨\nè¯¥è®ºæ–‡åœ¨ HuggingFace ç¤¾åŒºè·å¾—äº† {paper.upvotes} ä¸ªèµã€‚\n\n"
        f"## ğŸ·ï¸ å…³é”®è¯æ ‡ç­¾\nAI, ML"
    )


async def analyze_papers(
    papers: list["Paper"],
    target_date: date,
) -> list[str]:
    """Analyse multiple papers concurrently.

    Returns a list of analysis strings aligned with the input list.
    """
    client = _build_client()
    sem = asyncio.Semaphore(3)  # limit concurrency to avoid rate-limits

    async def _run(paper: "Paper") -> str:
        async with sem:
            return await analyze_paper(paper, client, target_date)

    results = await asyncio.gather(*[_run(p) for p in papers])
    return list(results)
